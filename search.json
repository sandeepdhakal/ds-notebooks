[
  {
    "objectID": "notebooks/qld-election-2024-pref-flows.html",
    "href": "notebooks/qld-election-2024-pref-flows.html",
    "title": "Visualising preference flows in 2024 QLD state elections",
    "section": "",
    "text": "I have recently written a dashboard to explore the preference flows of votes during the 2024 QLD state elections. The dashboard is available here.\nThe dashboard has three tabs: 1. Overall preference flows, 2. Preference flows in each electorate, 3. A what-if scenario to test the impact of different levels of preference flows from the Greens to Labor.\nIn this noitebook, I will discuss how I generated the overall preference flow Sankey diagram and a corresponding table."
  },
  {
    "objectID": "notebooks/qld-election-2024-pref-flows.html#preference-count-for-each-electorate",
    "href": "notebooks/qld-election-2024-pref-flows.html#preference-count-for-each-electorate",
    "title": "Visualising preference flows in 2024 QLD state elections",
    "section": "Preference count for each electorate",
    "text": "Preference count for each electorate\nThe first preference votes, preference flows and the final result are all available with a single API call.\n\nhttps://resultsdata.elections.qld.gov.au/SGE2024-preference-count-district-{electorate_stub}.json\n\nLet’s see what it looks like for the Aspley electorate.\n\nfrom string import Template\n\nprefs_url = Template(\n    \"https://resultsdata.elections.qld.gov.au/SGE2024-preference-count-district-$electorate.json\"\n)\nelectorate = \"aspley\"\nres = requests.get(prefs_url.substitute(electorate=electorate))\nresult = res.json()\n\npprint.pp(result, depth=2)\n\n{'contest': 'state',\n 'electorateName': 'Aspley',\n 'countround': 4,\n 'lastUpdated': '2024-11-13T14:43:05.097649+10:00',\n 'totalFormalVotes': 35747,\n 'totalVotes': 35747,\n 'candidates': [{...}, {...}],\n 'totalPrimary': 35747,\n 'preferenceDistributionDetails': {'distributions': [...], 'primary': [...]},\n 'totalExhausted': 0,\n 'totalExhaustedPercentage': '0.00%',\n 'votesRemainingInCount': 35747,\n 'votesRemainingInCountPercentage': '100.00%'}\n\n\nThis json file has a lot of information. So let’s break this down into different chunks.\n\nPrimary votes\nThe primary votes obtained by each candidate are provided as preferenceDistributionDetails &gt; primary. This is a list containing information for each candidate.\n\nresult[\"preferenceDistributionDetails\"][\"primary\"]\n\n[{'ballotName': 'HALL, Allan',\n  'ballotOrderNumber': 1,\n  'party': \"Pauline Hanson's One Nation Queensland Division\",\n  'partyCode': 'One Nation',\n  'primary': 1539},\n {'ballotName': 'CAPELL, Wayne',\n  'ballotOrderNumber': 2,\n  'party': 'Family First Queensland',\n  'partyCode': 'Family First',\n  'primary': 707},\n {'ballotName': 'MELLISH, Bart',\n  'ballotOrderNumber': 3,\n  'party': 'Australian Labor Party (State of Queensland)',\n  'partyCode': 'Australian Labor Party',\n  'primary': 13988},\n {'ballotName': 'COOPER, Amanda',\n  'ballotOrderNumber': 4,\n  'party': 'Liberal National Party of Queensland',\n  'partyCode': 'LNP',\n  'primary': 15696},\n {'ballotName': 'HAWKINS, Fiona',\n  'ballotOrderNumber': 5,\n  'party': 'Queensland Greens',\n  'partyCode': 'The Greens',\n  'primary': 3817}]\n\n\n\n\nFinal tally\nThe final tally is available as the candidates key in the JSON result.\n\nresult[\"candidates\"]\n\n[{'ballotName': 'MELLISH, Bart',\n  'ballotOrderNumber': 3,\n  'party': 'Australian Labor Party (State of Queensland)',\n  'partyCode': 'Australian Labor Party',\n  'count': 17889,\n  'percentage': '50.04%'},\n {'ballotName': 'COOPER, Amanda',\n  'ballotOrderNumber': 4,\n  'party': 'Liberal National Party of Queensland',\n  'partyCode': 'LNP',\n  'count': 17858,\n  'percentage': '49.96%'}]\n\n\n\n\nPreference distributions\nThis contains information about each preference distribution: the excluded candidate, preferences distributed to each remaining candidate, and the new tally of the remaining candidates.\n\npprint.pp(result[\"preferenceDistributionDetails\"][\"distributions\"][0], depth=3)\n\n{'exclusion': 1,\n 'excludedCandidate': 'CAPELL, Wayne',\n 'excludedCandidateBallotOrder': 2,\n 'excludedCandidateParty': 'Family First Queensland',\n 'excludedCandidatePartyCode': 'Family First',\n 'excludedCandidateVotes': 707,\n 'exhausted': 0,\n 'exhaustedPercentage': '0.00%',\n 'totalExhausted': 0,\n 'votesDistributed': 707,\n 'votesRemainingInCount': 35747,\n 'preferences': [{'ballotName': 'HALL, Allan',\n                  'ballotOrderNumber': 1,\n                  'party': \"Pauline Hanson's One Nation Queensland Division\",\n                  'partyCode': 'One Nation',\n                  'preferences': 302,\n                  'preferencesPercentage': '42.72%',\n                  'runningTotal': 1841},\n                 {'ballotName': 'MELLISH, Bart',\n                  'ballotOrderNumber': 3,\n                  'party': 'Australian Labor Party (State of Queensland)',\n                  'partyCode': 'Australian Labor Party',\n                  'preferences': 113,\n                  'preferencesPercentage': '15.98%',\n                  'runningTotal': 14101},\n                 {'ballotName': 'COOPER, Amanda',\n                  'ballotOrderNumber': 4,\n                  'party': 'Liberal National Party of Queensland',\n                  'partyCode': 'LNP',\n                  'preferences': 179,\n                  'preferencesPercentage': '25.32%',\n                  'runningTotal': 15875},\n                 {'ballotName': 'HAWKINS, Fiona',\n                  'ballotOrderNumber': 5,\n                  'party': 'Queensland Greens',\n                  'partyCode': 'The Greens',\n                  'preferences': 113,\n                  'preferencesPercentage': '15.98%',\n                  'runningTotal': 3930}]}"
  },
  {
    "objectID": "notebooks/qld-election-2024-pref-flows.html#downloading-all-data",
    "href": "notebooks/qld-election-2024-pref-flows.html#downloading-all-data",
    "title": "Visualising preference flows in 2024 QLD state elections",
    "section": "Downloading all data",
    "text": "Downloading all data\nNext, we want to dowload all the data and save that somewhere to avoid repeating the API calls.\nBefore doing that, lets also assign colours to the parties. These are based on the colours commonly associated with these parties. https://peo.gov.au/understand-our-parliament/your-questions-on-notice/questions/what-are-the-colours-of-the-australian-political-parties\nI’ve assigned unique colours to other parties whose colours I’m not aware of. And we also need colours for independents.\n\ncolours = {\n    \"ALP\": \"#FF0000\",\n    \"Family First\": \"#1CE6EF\",\n    \"KAP\": \"#8B0000\",\n    \"LCQP\": \"#788D66\",\n    \"LNP\": \"#0000FF\",\n    \"One Nation\": \"#FFA500\",\n    \"The Greens\": \"#00FF00\",\n    \"Animal Justice Party\": \"#885578\",\n    \"Libertarians\": \"#FF34FF\",\n}\n\n# For independent candidates\nother_colours = [\n    \"#000000\",\n    \"#FFFF00\",\n    \"#FAD09F\",\n    \"#FF8A9A\",\n    \"#D157A0\",\n    \"#FF4A46\",\n]\n\nBased on the structure of the results we saw above and also the requirements for this project, we will create three dataframes focusing on the first preferences, final result, and the distributions.\nWe’ll use pandas, particularly the json_normalize and the DataFrame.from_records - based on the JSON structure provided by the API.\n\nimport pandas as pd\n\nLet’s specify what keys we are interested in.\n\nexclusion_cols = [\n    \"exclusion\",\n    \"excludedCandidate\",\n    \"excludedCandidatePartyCode\",\n    \"excludedCandidateBallotOrder\",\n    \"votesDistributed\",\n]\npref_cols = [\n    \"ballotName\",\n    \"preferences\",\n    \"preferencePercentage\",\n    \"runningTotal\",\n    \"partyCode\",\n    \"ballotOrderNumber\",\n]\n\nWe’ll define a couple of methods to parse the data from the JSON.\nThe first method we’ll define is to parse the votes received by candidates in either the primary count or the final count. We have already seen the strucutre of this data above. One key difference between the primary and final counts is the keys used to denote the votes: primary and count respectively. We’ll get around this by rename the primary column to count (if it exists) in the dataframe.\n\ndef parse_candidate_votes(data):\n    \"\"\"Parse the votes received by each candidate in the `data`.\"\"\"\n    counts = pd.DataFrame.from_records(data, exclude=[\"party\"]).rename(\n        columns={\n            \"ballotName\": \"candidate\",\n            \"ballotOrderNumber\": \"ballotOrder\",\n            \"partyCode\": \"party\",\n            \"primary\": \"count\",  # rename 'primary' to 'count' if it exists\n        }\n    )\n    # Now we have a 'count' column irrespective of the data\n\n    # assign colours to the candidates based on their party\n    counts[\"colour\"] = counts[\"party\"].map(colours)\n\n    # independents without a party colour\n    no_colour = counts[\"colour\"].isna()\n    counts.loc[no_colour, [\"colour\"]] = other_colours[: no_colour.sum()]\n    return counts\n\nThe next method is to parse the preference distributions in each distribution round. This will take a list of distributions and return a dataframe.\n\ndef parse_distribution(data):\n    distribution = pd.json_normalize(data, \"preferences\", exclusion_cols)\n\n    # remove redundant columns\n    distribution = distribution.loc[\n        :, distribution.columns.isin(exclusion_cols + pref_cols)\n    ]\n\n    # rename columns\n    distribution = distribution.rename(\n        columns={\n            \"ballotName\": \"toCandidate\",\n            \"ballotOrderNumber\": \"toBallotOrder\",\n            \"partyCode\": \"toParty\",\n            \"runningTotal\": \"toRunningTotal\",\n            \"excludedCandidate\": \"fromCandidate\",\n            \"excludedCandidatePartyCode\": \"fromParty\",\n            \"excludedCandidateBallotOrder\": \"fromBallotOrder\",\n        }\n    )\n    return distribution\n\nNow, we’re ready to download the data.\n\nfirst_prefs, distributions, final_tallies = [], [], []\nfor electorate in [x[\"stub\"] for x in electorates]:\n    res = requests.get(prefs_url.substitute(electorate=electorate))\n    result = res.json()\n\n    # first preferences\n    first_pref = parse_candidate_votes(\n        result[\"preferenceDistributionDetails\"][\"primary\"]\n    )\n    first_pref[\"electorate\"] = electorate\n    first_pref = first_pref.sort_values(\"count\")\n    first_prefs.append(first_pref)\n\n    # final tally\n    final = parse_candidate_votes(result[\"candidates\"])\n    final[\"electorate\"] = electorate\n    final = final.sort_values(\"count\")\n    final_tallies.append(final)\n\n    # preference distribution\n    data = result[\"preferenceDistributionDetails\"][\"distributions\"]\n    distribution = parse_distribution(data)\n    distribution[\"electorate\"] = electorate  # add electorate information\n    distributions.append(distribution)"
  },
  {
    "objectID": "notebooks/qld-election-2024-pref-flows.html#tidy",
    "href": "notebooks/qld-election-2024-pref-flows.html#tidy",
    "title": "Visualising preference flows in 2024 QLD state elections",
    "section": "Tidy",
    "text": "Tidy\nLet’s tidy the data.\n\nConcatenate data for each electorate into a single dataframe\nSet appropriate data types\nRename ‘Australian Labor Party’ to ‘ALP’\nSet ‘IND’ as the party value for independents - it is currently an empty string\n\n\n# we'll concatenate all electorate data into one dataframe for each data type\ndistribution = pd.concat(distributions).set_index(\"electorate\")\nfirst_pref = pd.concat(first_prefs).set_index(\"electorate\")\nfinal_tally = pd.concat(final_tallies)\n\n# set appropriate data types for each dataframe\ndistribution = distribution.astype(\n    {\n        \"toBallotOrder\": \"uint8\",\n        \"preferences\": \"uint16\",\n        \"toRunningTotal\": \"uint16\",\n        \"exclusion\": \"uint8\",\n        \"fromBallotOrder\": \"uint8\",\n        \"votesDistributed\": \"uint16\",\n        \"toParty\": \"category\",\n        \"fromParty\": \"category\",\n    }\n)\n\nfirst_pref = first_pref.astype(\n    {\n        \"ballotOrder\": \"uint8\",\n        \"count\": \"uint16\",\n        \"party\": \"category\",\n        \"colour\": \"category\",\n    }\n)\n\nfinal_tally = final_tally.astype(\n    {\n        \"ballotOrder\": \"uint8\",\n        \"party\": \"category\",\n        \"count\": \"uint16\",\n        \"colour\": \"category\",\n    }\n)\n\n# rename Australian Labor Party to ALP\ndistribution[\"toParty\"] = distribution[\"toParty\"].cat.rename_categories(\n    {\"Australian Labor Party\": \"ALP\"}\n)\ndistribution[\"fromParty\"] = distribution[\"fromParty\"].cat.rename_categories(\n    {\"Australian Labor Party\": \"ALP\"}\n)\nfinal_tally[\"party\"] = final_tally[\"party\"].cat.rename_categories(\n    {\"Australian Labor Party\": \"ALP\"}\n)\nfirst_pref[\"party\"] = first_pref[\"party\"].cat.rename_categories(\n    {\"Australian Labor Party\": \"ALP\"}\n)\n\n# Change independent candidates' party from &lt;empty-string&gt; to \"Ind.\"\nIND = \"Ind.\"\n\ndistribution[\"toParty\"] = distribution[\"toParty\"].cat.rename_categories({\"\": IND})\ndistribution[\"fromParty\"] = distribution[\"fromParty\"].cat.rename_categories({\"\": IND})\n\nfinal_tally[\"party\"] = final_tally[\"party\"].cat.rename_categories({\"\": IND})\nfirst_pref[\"party\"] = first_pref[\"party\"].cat.rename_categories({\"\": IND})"
  },
  {
    "objectID": "notebooks/matplotlib-animation-with-legend.html",
    "href": "notebooks/matplotlib-animation-with-legend.html",
    "title": "Animated scatter plot with size legend with matplotlib",
    "section": "",
    "text": "I have been looking at generating animated plots using Matplotlib, with the official documentation providing an excellent example for basic animations.\nHowever, I couldn’t find any good examples of generating animations with legends for size of the marker in a scatter plot. I had been looking to animate the time and magnitude of the 25th April 2015 earthquake of Nepal along with its foreshocks and aftershocks.\nIn this post, I will explain how I managed to achieve that (with help from some excellent references online, of course)."
  },
  {
    "objectID": "notebooks/matplotlib-animation-with-legend.html#getting-the-earthquake-data",
    "href": "notebooks/matplotlib-animation-with-legend.html#getting-the-earthquake-data",
    "title": "Animated scatter plot with size legend with matplotlib",
    "section": "Getting the earthquake data",
    "text": "Getting the earthquake data\nThe first step is of course to the get the data about the earthquakes. This is easily available using the USGS Catalog. I used geopandas to directly read a GeoDataFrame from the catalog’s API, but I won’t go into a lot of details here; the code below should be self explanatory.\nOne important thing to note is that the catalog API doesn’t support filtering by country. It only supports filtering by latitutde and longitude bounds (west, south, east, north). After this, I used the within method of geopandas to limit the results to earthquakes that occured only in Nepal. The within method requires a Polygon object. For this, I directly downloaded a GeoJSON file of the administrative boundaries from the geoBoundaries project on GitHub.\n\nimport urllib.parse\n\nimport geopandas as gpd\nimport pandas as pd\n\n# these are the rough bounds of the earthquake and its aftershocks\nminlatitude, maxlatitude = 27, 29\nminlongitude, maxlongitude = 84, 87\n\n# limiting the search to April and May 2015, and only those &gt;= 5 magnitude\nCATALOG_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query.geojson\"\nparams = {\n    \"starttime\": \"2015-04-01\",\n    \"endtime\": \"2015-05-31\",\n    \"maxlatitude\": maxlatitude,\n    \"minlatitude\": minlatitude,\n    \"maxlongitude\": maxlongitude,\n    \"minlongitude\": minlongitude,\n    \"minmagnitude\": 5,\n    \"eventtype\": \"earthquake\",\n    \"orderby\": \"time\",\n}\ncolumns = [\"mag\", \"time\", \"geometry\"]\n\nurl = f\"{CATALOG_URL}?{urllib.parse.urlencode(params)}\"\nquakes = gpd.read_file(url, columns=columns)\n\n# get the national boundary of Nepal\nboundary_url = \"https://media.githubusercontent.com/media/wmgeolab/geoBoundaries/refs/heads/main/releaseData/gbOpen/NPL/ADM0/geoBoundaries-NPL-ADM0.geojson\"\nnepal = gpd.read_file(boundary_url)\n\n# ensure both dataframes are in the same CRS\nquakes = quakes.to_crs(nepal.crs)\n\n# filter earthquakes to those within Nepal's boundaries\nquakes = quakes[quakes.within(nepal.iloc[0]['geometry'])]\n\n# sort from earliest to latest\n# localize the datetime to Nepal's datetime\nquakes = quakes.sort_values(\"time\", ascending=True, ignore_index=True)\nquakes[\"time\"] = pd.to_datetime(quakes[\"time\"], unit=\"ms\").dt.tz_localize(\n    tz=\"Asia/Kathmandu\",\n)\n\n# for working with matplotlib, add separate latitude/longitude columns\nquakes = quakes.join(\n    quakes.get_coordinates().rename(columns=dict(y=\"latitude\", x=\"longitude\"))\n)\n\nNow, lets look at what this data looks like, and then do a simple visualisation with geopandas. We have 30 earthquakes &gt;= 5 magnitude.\n\nquakes.head()\n\n\n\n\n\n\n\n\nmag\ntime\ngeometry\nlongitude\nlatitude\n\n\n\n\n0\n7.8\n2015-04-25 06:11:25.950000+05:45\nPOINT Z (84.7314 28.2305 8.22)\n84.7314\n28.2305\n\n\n1\n6.1\n2015-04-25 06:15:22.910000+05:45\nPOINT Z (85.5398 27.6285 10)\n85.5398\n27.6285\n\n\n2\n5.6\n2015-04-25 06:18:10.870000+05:45\nPOINT Z (86.0213 27.6857 10)\n86.0213\n27.6857\n\n\n3\n5.4\n2015-04-25 06:20:40.340000+05:45\nPOINT Z (84.492 28.2046 10)\n84.4920\n28.2046\n\n\n4\n5.1\n2015-04-25 06:22:02.750000+05:45\nPOINT Z (85.1141 27.8006 10)\n85.1141\n27.8006\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(6,4))\nnepal.plot(color=\"white\", edgecolor=\"black\", ax=ax)\nquakes.plot(ax=ax, markersize='mag', color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\nOf course, some of these earthquakes might not be foreshocks or aftershocks, but that shouldn’t matter for the purpose of this post. The main goal is to animate the scatter plot."
  },
  {
    "objectID": "notebooks/matplotlib-animation-with-legend.html#generating-the-animated-plot",
    "href": "notebooks/matplotlib-animation-with-legend.html#generating-the-animated-plot",
    "title": "Animated scatter plot with size legend with matplotlib",
    "section": "Generating the animated plot",
    "text": "Generating the animated plot\nMy goals for the plot were to:\n\nanimate the time the earthquake occurred,\nmarker sizes indicating the earthquake’s magnitude,\nadd a legend for the magnitude.\n\n\nMarker size for magnitudes\nEarthquake magnitudes are logarithmic in nature; for example, a magnitude 7 earthquake is 10 times stronger than a magnitude 6 one. To visualise this, I have used the following formula to calculate the marker size for the earthquakes.\n\\(marker\\_size = base\\_size \\times 10^{(magnitude - base\\_magnitude)}\\)\n\nmin_marker_size = 10\nmin_magnitude = 5\nquakes[\"marker_size\"] = min_marker_size * (10 ** (quakes[\"mag\"] - min_magnitude))\n\nquakes.head()\n\n\n\n\n\n\n\n\nmag\ntime\ngeometry\nlongitude\nlatitude\nmarker_size\n\n\n\n\n0\n7.8\n2015-04-25 06:11:25.950000+05:45\nPOINT Z (84.7314 28.2305 8.22)\n84.7314\n28.2305\n6309.573445\n\n\n1\n6.1\n2015-04-25 06:15:22.910000+05:45\nPOINT Z (85.5398 27.6285 10)\n85.5398\n27.6285\n125.892541\n\n\n2\n5.6\n2015-04-25 06:18:10.870000+05:45\nPOINT Z (86.0213 27.6857 10)\n86.0213\n27.6857\n39.810717\n\n\n3\n5.4\n2015-04-25 06:20:40.340000+05:45\nPOINT Z (84.492 28.2046 10)\n84.4920\n28.2046\n25.118864\n\n\n4\n5.1\n2015-04-25 06:22:02.750000+05:45\nPOINT Z (85.1141 27.8006 10)\n85.1141\n27.8006\n12.589254\n\n\n\n\n\n\n\n\n\nA basic animated scatter plot\nGenerating a simple animated scatter plot in Matplotlib is quite easy. Let’s do that to start with.\n\nimport contextily as cx\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(layout=\"constrained\", figsize=(6,5))\nax.set(xlim=[minlongitude, maxlongitude], ylim=[minlatitude, maxlatitude])\nax.set_title(\"The April 2015 Nepal earthquake and its aftershocks\")\n\n# add basemap\ncx.add_basemap(ax, crs=quakes.crs, zoom=8, source=\"CartoDB.Voyager\")\n\n# the first point in the animation\npoints_opts = dict(alpha=0.5, linewidth=0, color=\"red\")\npoints = ax.scatter(\n    quakes[\"longitude\"].iloc[0],\n    quakes[\"latitude\"].iloc[0],\n    s=quakes[\"marker_size\"].iloc[0],\n    **points_opts\n)\n\n# the first label\n# labels show the time of the earthquake\nlabel_opts = dict(ha=\"center\", va=\"center\", fontsize=15, color=\"black\", transform=ax.transAxes)\nlabel = ax.text(\n    0.25,\n    0.95,\n    quakes[\"time\"].iloc[0].strftime(\"%Y-%m-%d %H:%M\"),\n    **label_opts\n)\n\n# loop through each point in the data and draw a marker and legend for that data point\ndef update(frame):\n    # for each frame, update the data stored on each artist.\n    x = quakes[\"longitude\"].iloc[:frame]\n    y = quakes[\"latitude\"].iloc[:frame]\n\n    # update the scatter plot:\n    data = np.stack([x, y]).T\n    points.set_offsets(data)\n\n    # set the size of the earthquakes\n    points.set_sizes(quakes[\"marker_size\"].iloc[:frame])\n\n    # update the label\n    label.set_text(quakes[\"time\"].iloc[frame - 1].strftime(\"%Y-%m-%d %H:%M\"))\n\n    return (points,)\n\n\n# animate by looping through all datapoints.\nani = animation.FuncAnimation(\n    fig=fig, func=update, frames=len(quakes), interval=600\n)\n\n# To save the animation using Pillow as a gif\nwriter = animation.PillowWriter(fps=10, bitrate=1800)\nani.save(\"animated-scatter-v1.gif\", writer=writer)\n\n\nplt.ioff\nplt.close()\n\nThis gives us a nice animated scatter plot.\n\nBut the legend is missing, without which the marker sizes don’t have much meaning. I found adding the correct legend to be the most difficult part of this process because there aren’t many working examples of this on the WWW.\n\n\nCustom legend\nThe solution that I ended up using is to add a custom legend that doesn’t completely correspond with the data being plotted. This is because I wanted to show the magnitude legend from the get go even when only a small part of the data has been plotted initially. I used the example in the official documentation as a starting point.\nI want the legend to have 4 different sizes: 5, 6, 7, and 7.8 (the strongest earthquake). My first attempt was to generate the marker sizes as I did above and generate the legend. I added a Line2D object for each marker size, and use the collection of those Line2D objects to create the legend.\n\nfrom matplotlib.lines import Line2D\n\n# the four magnitudes in the legend\nmags = np.array([5, 6, 7, quakes['mag'].max()])\nmarker_sizes = min_marker_size * (10 ** (mags - min_magnitude))\n\ndef get_legend_elements(marker_sizes):\n    # marker size for each magnitude\n    marker_opts = dict(marker=\"o\", color=\"w\", markerfacecolor=\"red\", alpha=0.5)\n    legend_elements = [\n        Line2D(\n            [0],\n            [i],\n            label=mags[i],\n            markersize=s,\n            **marker_opts\n        )\n        for i, s in enumerate(marker_sizes)\n    ]\n    return legend_elements\n\nfig, ax = plt.subplots()\nlegend_opts = dict(title=\"Magnitude\", labelspacing=1, frameon=False)\nax.legend(handles=get_legend_elements(marker_sizes), **legend_opts, loc='center')\n\n\n\n\n\n\n\n\nThat did’t work as exepcted as the marker sizes were too big and didn’t align with the marker sizes on the scatter plot. I arrived at the solution after a lot of digging around and trial and error.\nLooking at the scatterplot documentation, it turns out that the marker size (s) is points ** 2. However, the marker size (markersize) in Line2D is only in points This means the marker size for the legend should be square roots of the marker size for the scatter plot. Perhaps I should have realised this much sooner.\nBelow is the 2nd version of the legend; and the sizes correspond with those in the plot.\n\n# marker size for each magnitude\nmarker_sizes = np.sqrt(min_marker_size * (10 ** (mags - min_magnitude)))\n\nfig, ax = plt.subplots()\nax.legend(\n    handles=get_legend_elements(marker_sizes),\n    loc=\"center\",\n    title=\"Magnitude\",\n    labelspacing=1,\n    frameon=False\n)\nplt.show()"
  },
  {
    "objectID": "notebooks/matplotlib-animation-with-legend.html#second-attempt",
    "href": "notebooks/matplotlib-animation-with-legend.html#second-attempt",
    "title": "Animated scatter plot with size legend with matplotlib",
    "section": "Second attempt",
    "text": "Second attempt\n\nfig, ax = plt.subplots(layout=\"constrained\", figsize=(7,5))\nax.set(xlim=[minlongitude, maxlongitude], ylim=[minlatitude, maxlatitude])\nax.set_title(\"The April 2015 Nepal earthquake and its aftershocks\")\ncx.add_basemap(ax, crs=quakes.crs, zoom=8, source=\"CartoDB.Voyager\")\n\npoints = ax.scatter(\n    quakes[\"longitude\"].iloc[0],\n    quakes[\"latitude\"].iloc[0],\n    s=quakes[\"marker_size\"].iloc[0],\n    **points_opts\n)\n\nlabel = ax.text(\n    0.25,\n    0.95,\n    quakes[\"time\"].iloc[0].strftime(\"%Y-%m-%d %H:%M\"),\n    **label_opts\n)\n\nax.legend(\n    handles=get_legend_elements(marker_sizes),\n    loc=\"center left\",\n    title=\"Magnitude\",\n    labelspacing=1,\n    frameon=False,\n    bbox_to_anchor=(1.05, 0.5), # place the legend to the right\n)\n\nani = animation.FuncAnimation(\n    fig=fig, func=update, frames=len(quakes), interval=600\n)\n\n# To save the animation using Pillow as a gif\nwriter = animation.PillowWriter(fps=15, bitrate=1800)\nani.save(\"animated-scatter-v2.gif\", writer=writer)\n\nplt.ioff\nplt.close()\n\nFinally, this gave me the animated plot that I wanted to create.\n."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sandeep Dhakal's Jupyter notebooks",
    "section": "",
    "text": "Spatial KDE plots in Python\n\n\n\n\n\n\nGIS\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnimated scatter plot with size legend with matplotlib\n\n\n\n\n\n\nanimation\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nZonal statistics: mean elevation of brisbane suburbs\n\n\n\n\n\n\nGIS\n\n\nDEM\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising preference flows in 2024 QLD state elections\n\n\n\n\n\n\nsankey\n\n\nelections\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Earthquakes in Nepal\n\n\n\n\n\n\nGIS\n\n\nearthquakes\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/brisbane-suburbs-mean-elevation.html",
    "href": "notebooks/brisbane-suburbs-mean-elevation.html",
    "title": "Zonal statistics: mean elevation of brisbane suburbs",
    "section": "",
    "text": "As part of my geospatial analysis journey and the mapping of various things in Brisbane, in this notebook I will look at the digital elevation data of Brisbane suburbs.\nI will tackle this as two sequential tasks:"
  },
  {
    "objectID": "notebooks/brisbane-suburbs-mean-elevation.html#clipping-to-brisbanes-geometry",
    "href": "notebooks/brisbane-suburbs-mean-elevation.html#clipping-to-brisbanes-geometry",
    "title": "Zonal statistics: mean elevation of brisbane suburbs",
    "section": "Clipping to Brisbane’s geometry",
    "text": "Clipping to Brisbane’s geometry\nNotice that since we only used the 4 geographical bounds to download the raster data, we will further need to mask it to Brisbane’s geographic boundary. We can get the mask using the dissolve method from geopandas. Let’s also filter by elevation above 0 meters, so we can focus on the land area.\n\n# shapely Polygon  to clip raster\n# use shapely polygon in clip method of rioxarray object to clip raster\ndem = dem.rio.clip(suburbs.dissolve().geometry)\n\n# while plotting, lets only show elevation higher than 0 metres above sea-level.\ndem.where(dem &gt; 0).plot(cmap=\"terrain\")\n\n\n\n\n\n\n\n\nWe can clearly see that most parts of Brisbane are low lying (under 100 metres above sea level) with north-western parts (such as England Creek, Lake Manchester, Mount Coot-tha, Enoggera Reservoir) with higher elevation.\nWe can also clip the data around the apparently low-lying parts to see the minute variations in elevation throughout the city, and the difference between the north-eastern and southern parts of the city.\n\nfrom shapely import Polygon\n\ntest_poly = [\n    Polygon(((152.975, -27.3), (152.975, -27.65), (153.2, -27.65), (153.2, -27.3)))\n]\nclipped_dem = dem.rio.clip(test_poly)\n\nfig, ax = plt.subplots()\nclipped_dem.where(clipped_dem &gt; 0).plot(cmap=\"terrain\", ax=ax)\nax.xaxis.set_major_locator(plt.MaxNLocator(5))\nplt.show()"
  },
  {
    "objectID": "notebooks/nepal-earthquakes.html",
    "href": "notebooks/nepal-earthquakes.html",
    "title": "Visualising Earthquakes in Nepal",
    "section": "",
    "text": "In this notebook, we will analyse the earthquakes that occurred in Nepal since 2010.\n\nGetting the Data\nThe USGS provides free access to the earthquake data via the Earthquake Catalog API.\nThe API supports a number of query parameters, but we need only a subset of those, particularly to limit the search to Nepal’s geographical bounds and since 2010. We’ll also limit the results to include earthquakes &gt; 3 in magnitude. It can return the data in a number of formats (including CSV and GeoJSON), here we’ll use the GeoJSON format.\nWe need to provide the geographical bounds (min/max latitude and longitude) of the area for which to get the data; the API doesn’t support filtering by country. To this end, let’s read a GeoJSON file of the administrative boundaries from the geoBoundaries project on GitHub.\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# get the national boundary of Nepal\nboundary_url = \"https://media.githubusercontent.com/media/wmgeolab/geoBoundaries/refs/heads/main/releaseData/gbOpen/NPL/ADM0/geoBoundaries-NPL-ADM0.geojson\"\nnepal = gpd.read_file(boundary_url)\n\nNow, we can read the data from the API.\n\nimport pandas as pd\nimport urllib.parse\nimport datetime\n\nbounds = nepal.bounds\nminx, miny, maxx, maxy = bounds.iloc[0].to_numpy()\n\nCATALOG_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query.geojson\"\nparams = {\n    \"starttime\": \"2010-01-01\",\n    \"endtime\": str(datetime.date.today()),\n    \"maxlatitude\": maxy,\n    \"minlatitude\": miny,\n    \"maxlongitude\": maxx,\n    \"minlongitude\": minx,\n    \"minmagnitude\": 4,\n    \"eventtype\": \"earthquake\",\n    \"orderby\": \"time\",\n}\ncolumns = [\"mag\", \"place\", \"time\", \"geometry\"]\n\nurl = f\"{CATALOG_URL}?{urllib.parse.urlencode(params)}\"\nquakes = gpd.read_file(url, columns=columns)\n\nquakes.head()\n\n\n\n\n\n\n\n\nmag\nplace\ntime\ngeometry\n\n\n\n\n0\n4.4\n148 km E of Saga, China\n1745838013982\nPOINT Z (86.7587 29.2963 10)\n\n\n1\n4.2\n138 km N of Lobuche, Nepal\n1745193803011\nPOINT Z (86.9874 29.1891 2.847)\n\n\n2\n4.2\n93 km ENE of Lobuche, Nepal\n1744755636029\nPOINT Z (87.7324 28.1352 71.393)\n\n\n3\n4.3\n81 km NE of Lobuche, Nepal\n1744365447572\nPOINT Z (87.2746 28.557 10)\n\n\n4\n4.2\n115 km N of Lobuche, Nepal\n1744261947380\nPOINT Z (86.6178 28.9806 10)\n\n\n\n\n\n\n\nLet’s make some changes to the quakes dataframe here:\n\nconvert the time to pandas’ datetime type. The time is currently in UTC, so we’ll also convert that to ‘Asia/Kathmandu’,\nset the CRS to be the same as the nepal geodataframe, WebMercator, and\nextract the latitude and longitude from the geometry to work better with matplotlib.\n\n\n# 1.\nquakes[\"time\"] = (\n    pd.to_datetime(quakes[\"time\"], unit=\"ms\")\n    .dt.tz_localize(\"UTC\")\n    .dt.tz_convert(\"Asia/Kathmandu\")\n)\n\n# 2.\nquakes = quakes.to_crs(nepal.crs)\n\n# 3.\nquakes = quakes.join(\n    quakes.get_coordinates().rename(columns=dict(x=\"longitude\", y=\"latitude\"))\n)\n\nquakes.head()\n\n\n\n\n\n\n\n\nmag\nplace\ntime\ngeometry\nlongitude\nlatitude\n\n\n\n\n0\n4.4\n148 km E of Saga, China\n2025-04-28 16:45:13.982000+05:45\nPOINT Z (86.7587 29.2963 10)\n86.7587\n29.2963\n\n\n1\n4.2\n138 km N of Lobuche, Nepal\n2025-04-21 05:48:23.011000+05:45\nPOINT Z (86.9874 29.1891 2.847)\n86.9874\n29.1891\n\n\n2\n4.2\n93 km ENE of Lobuche, Nepal\n2025-04-16 04:05:36.029000+05:45\nPOINT Z (87.7324 28.1352 71.393)\n87.7324\n28.1352\n\n\n3\n4.3\n81 km NE of Lobuche, Nepal\n2025-04-11 15:42:27.572000+05:45\nPOINT Z (87.2746 28.557 10)\n87.2746\n28.5570\n\n\n4\n4.2\n115 km N of Lobuche, Nepal\n2025-04-10 10:57:27.380000+05:45\nPOINT Z (86.6178 28.9806 10)\n86.6178\n28.9806\n\n\n\n\n\n\n\n\nax = nepal.boundary.plot(color=\"black\", linewidth=0.5)\nquakes.plot(ax=ax, markersize=5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFiltering the results\nWe can see that the results include all earthquakes located within the rectangular geogrpahical bounds we provided to the API; there are many earthquakes recorded in China and a few in India that we want to filter out.\nTo fix this, we’ll use the within function provided by geopandas to filter only those within Nepal’s boundary.\n\nquakes = quakes[quakes.within(nepal.iloc[0].geometry)]\nax = nepal.boundary.plot(color=\"black\", linewidth=0.5)\nquakes.plot(ax=ax, markersize=5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEarthquakes by recency and magnitude\nNow that we have filtered the earthquakes to those that occurred within Nepal’s boundaries, we can improve the above map by indicating:\n\nthe strength of the earthquake (magnitude): Earthquake magnitudes are logarithmic in nature; for example, a magnitude 7 earthquake is 10 times stronger than a magnitude 6 one. To visualise this, I have used the following formula to calculate the marker size for the earthquakes.\n\\(marker\\_size = base\\_size \\times 10^{(magnitude - base\\_magnitude)}\\)\nwhen the earthquake took place (time): We’ll use the RdPu colormap; the points for earlier earthquakes are lighter and the more recent ones are dark purple. And we’ll also bin the time by year, so in this case we have a segmented colormap.\n\nInstead of the default plotting feature of geopandas we’ll utilise the flexibility of matplotlib to achieve the above.\n\nbase_size = 1\nbase_magnitude = 4\nmarker_sizes = base_size * (10 ** (quakes[\"mag\"] - base_magnitude))\n\n\nimport numpy as np\nimport matplotlib as mpl\n\n# use only the year - for a segmented colormap\nquakes.loc[:, \"year\"] = quakes[\"time\"].dt.year\n\n# customise the colormap for our year extents\ncmap = plt.get_cmap(\"RdPu\")\nbounds = sorted(quakes[\"year\"].unique())\nnorm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n\nfig, ax = plt.subplots(figsize=(7, 7))\nnepal.boundary.plot(color=\"black\", linewidth=0.5, ax=ax)\n\n# draw the point plot on the map\npoints = ax.scatter(\n    quakes[\"longitude\"],\n    quakes[\"latitude\"],\n    s=marker_sizes,\n    alpha=0.5,\n    c=quakes[\"year\"],\n    cmap=cmap,\n    norm=norm,\n)\n\n# produce a legend with a cross-section of sizes from the scatter plot\n# 1. here we reverse the size function from above so the legend values match the magnitude\nkw = dict(\n    prop=\"sizes\",\n    num=4,\n    color=points.cmap(0.7),\n    func=lambda s: np.log10(s / base_size) + base_magnitude,\n)\n\n# 2. then we create a custom legend for the size\nlegend = ax.legend(\n    *points.legend_elements(**kw),\n    loc=\"lower left\",\n    title=\"Magnitude\",\n    ncols=4,\n    frameon=False,\n)\n\n# 3. and finally add our legend to the plot\nax.add_artist(legend)\n\n# create a colorbar corresponding to the year bins\n# 1. we create an inset axis so we can place the colorbar within the map frame, instead of the default placement outside the frame\ncax = ax.inset_axes([0.45, 0.9, 0.5, 0.04])\n\n# 2. use the segmented colormap and normalisation created earlier\ncb = fig.colorbar(\n    mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n    cax=cax,\n    fraction=0.02,\n    shrink=0.5,\n    label=\"Year\",\n    location=\"bottom\",\n    ticks=[2010, 2013, 2016, 2019, 2022, 2025],\n)\ncb.ax.xaxis.set_label_position(\"top\")\n\nax.set_title(\"Earthquakes in Nepal since 2010: year and magnitude\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can also visualise distribution of the earthquakes by time and magnitude. We can see that, with the exception of the earthquakes in 2015, most earthqukes are in the 4-5 magnitude range.\n\nimport seaborn as sns\n\ngrid = sns.jointplot(\n    quakes,\n    x=\"time\",\n    y=\"mag\",\n    kind=\"hist\",\n    height=5,\n    ratio=3,\n    marginal_ticks=True,\n    palette=\"Set2\",\n    cmap=\"viridis\",\n)\n\n# jointplots are square in seaborn, we need a hacky way to change the dimensions.\ngrid.fig.set_figheight(4)\ngrid.fig.set_figwidth(6)\nplt.show()\n\n\n\n\n\n\n\n\nGiven the unusually high number of earthquakes in 2015, let’s remove the data for 2015, and see what the rest looks like. Visually, there doesn’t seem any significant variation in the number of earthquakes observed in a year.\n\nwithout_2015 = quakes[quakes[\"year\"] != 2015]\n\nfig, (ax1, ax2) = plt.subplots(\n    nrows=1, ncols=2, figsize=(5, 5), width_ratios=[3, 1], layout=\"constrained\"\n)\nsns.barplot(\n    without_2015.groupby([\"year\"]).size().reset_index(name=\"count\"),\n    y=\"year\",\n    x=\"count\",\n    orient=\"y\",\n    ax=ax1,\n)\nsns.boxplot(without_2015, y=\"mag\", ax=ax2, showmeans=True)\n\nfig.suptitle(\"Distribution of earthquakes recorded (except 2015)\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe 2015 earthquake\nFinally, let’s look at the 2015 earthquake. For this purpose, we’ll filter the earthquakes between April-May 2015. We’ll also roughly filter only those within certain bounds.\n\nfrom shapely import Polygon\n\ncoords = ((84, 29), (84, 27), (87, 27), (87, 29))\nquakes_2015 = quakes.set_index(\"time\").sort_index().loc[\"2015-04-01\":\"2015-05-31\"]\nquakes_2015 = quakes_2015[quakes_2015.within(Polygon(coords))]\n\nquakes_2015 = quakes_2015.sort_index()\n\nLet’s see how many earthquakes occurred each day during this period. And we can see that there were 71 earthquakes above 4 magnitude on the first day and 31 on the second day. On 05/12 when the largest aftershock happened, there were 51 earthquakes.\n\nquakes_2015[\"mag\"].groupby(pd.Grouper(freq=\"D\")).count().sort_values(\n    ascending=False\n).head(10)\n\ntime\n2015-04-25 00:00:00+05:45    71\n2015-05-12 00:00:00+05:45    51\n2015-04-26 00:00:00+05:45    31\n2015-05-13 00:00:00+05:45    15\n2015-04-27 00:00:00+05:45    11\n2015-05-02 00:00:00+05:45     8\n2015-04-29 00:00:00+05:45     5\n2015-05-04 00:00:00+05:45     5\n2015-05-16 00:00:00+05:45     4\n2015-05-11 00:00:00+05:45     3\nName: mag, dtype: int64\n\n\nFinally, lets plot the number and intensity of the earthquakes on 04/25, such that we can easily visualise when the earthquakes occurred.\nA normal barplot with matplotlib will not space out the bars based on the time of day - all bars will be evenly spaced. To achieve this, we will use pandas to set the frequency of the timestamp to 1 seconds and fill missing values with NaN.\n\n# first day\ns = quakes_2015.loc[\"2015-04-25\", [\"mag\"]]\n\n# set time freq to seconds\ns.index = s.index.ceil(freq=\"s\")\n\ns1 = s.asfreq(\"1s\")\n\n\nfrom matplotlib.dates import DateFormatter\n\nfig, ax = plt.subplots(figsize=(8, 0.5))\n\nplt.bar(\n    x=s1.index.tz_localize(None),\n    height=s1.mag,\n    width=0.5 * 1 / (24 * 60 * 60),\n    edgecolor=\"red\",\n)\n\n# Define the date format\ndate_form = DateFormatter(\"%-I %p\")\nax.xaxis.set_major_formatter(date_form)\nax.set_ylabel(\"Mag.\")\n\nax.set_title(\"Aftershocks within 24 hours of the 2025/04/15 11:56 AM earthquake\")\nplt.show()\n\n\n\n\n\n\n\n\nAnd below is a slight variation of the above as a dense time series heatmap.\n\nimport colorcet as cc\n\nfig, ax = plt.subplots(figsize=(8, 0.5))\n\ncmap = plt.get_cmap(\"cet_fire\")\nmin_mag, max_mag = 0, s1.mag.max()\nrescale = lambda y: (y - min_mag) / (max_mag - min_mag)\ncolours = cmap(rescale(s1.mag))\nbars = plt.bar(\n    x=s1.index.tz_localize(None), height=1, width=0, linewidth=1, edgecolor=colours\n)\n\n# Define the date format\ndate_form = DateFormatter(\"%-I %p\")\nax.xaxis.set_major_formatter(date_form)\nax.set_facecolor(\"black\")\n\nax.set_yticks([])\n\n# colorbar\nnorm = plt.Normalize(min_mag, max_mag)\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax, aspect=5)\ncbar.ax.xaxis.set_label_position(\"top\")\n\nax.set_title(\"Aftershocks within 24 hours of the 2025/04/15 11:56 AM earthquake\")\nplt.show()"
  },
  {
    "objectID": "notebooks/spatial-kde-plot.html",
    "href": "notebooks/spatial-kde-plot.html",
    "title": "Spatial KDE plots in Python",
    "section": "",
    "text": "I frequently use KDE plots for my work, but I have not previously used them for spatial analysis. I found a really cool example here using the geoplot Python library. geoplot uses Seaborn behind the scenes to generate the KDE plots.\nUnfortunately, geoplot didn’t work for me and I set about finding my own solution. In this post, I explain how I acheived that."
  },
  {
    "objectID": "notebooks/spatial-kde-plot.html#getting-individual-contour-regions",
    "href": "notebooks/spatial-kde-plot.html#getting-individual-contour-regions",
    "title": "Spatial KDE plots in Python",
    "section": "Getting individual contour regions",
    "text": "Getting individual contour regions\nSeaborn’s kdeplot returns a collection of children, including a QuadContourSet instance that contains information about the contour and contour regions.\n\nkde = sns.kdeplot(\n    fatal_crashes,\n    x=fatal_crashes.geometry.x,\n    y=fatal_crashes.geometry.y,\n    weights=\"Count_Casualty_Fatality\",\n    cmap=\"viridis\",\n)\n\n\n\n\n\n\n\n\n\nimport pprint\n\npprint.pp(kde.get_children())\n\n[&lt;matplotlib.contour.QuadContourSet object at 0x7fe103723800&gt;,\n &lt;matplotlib.spines.Spine object at 0x7fe10353c920&gt;,\n &lt;matplotlib.spines.Spine object at 0x7fe10353e750&gt;,\n &lt;matplotlib.spines.Spine object at 0x7fe10357ea80&gt;,\n &lt;matplotlib.spines.Spine object at 0x7fe10357ee70&gt;,\n &lt;matplotlib.axis.XAxis object at 0x7fe103a38500&gt;,\n &lt;matplotlib.axis.YAxis object at 0x7fe10357f080&gt;,\n Text(0.5, 1.0, ''),\n Text(0.0, 1.0, ''),\n Text(1.0, 1.0, ''),\n &lt;matplotlib.patches.Rectangle object at 0x7fe103723f20&gt;]"
  },
  {
    "objectID": "notebooks/spatial-kde-plot.html#clipping-contour-regions-to-boundaries",
    "href": "notebooks/spatial-kde-plot.html#clipping-contour-regions-to-boundaries",
    "title": "Spatial KDE plots in Python",
    "section": "Clipping contour regions to boundaries",
    "text": "Clipping contour regions to boundaries\nAnd each QuadCountourSet instance is a collection of contour paths, which we can access with the get_paths method. We can generate a shapely Polygon using the to_polygons method.\n\n# credit to https://towardsdatascience.com/from-kernel-density-estimation-to-spatial-analysis-in-python-64ddcdb6bc9b/\n\nfrom shapely import Polygon\nfrom matplotlib.contour import QuadContourSet\n\ncontour_set = None\nfor i in kde.get_children():\n    if type(i) is QuadContourSet:\n        contour_set = i\n        break\n\ncontour_paths = []\n# Loop through all contours\nfor contour in contour_set.get_paths():\n    # Create a polygon for the countour\n    # First polygon is the main countour, the rest are holes\n    for ncp, cp in enumerate(contour.to_polygons()):\n        x = cp[:, 0]\n        y = cp[:, 1]\n        new_shape = Polygon([(i[0], i[1]) for i in zip(x, y)])\n        if ncp == 0:\n            poly = new_shape\n        else:\n            # Remove holes, if any\n            poly = poly.difference(new_shape)\n\n    # Append polygon to list\n    contour_paths.append(poly)\n\n# make holes corresponding to the inner contours\ncontour_polys = [\n    contour_paths[i].difference(contour_paths[i + 1])\n    for i in range(len(contour_paths) - 2)\n] + [contour_paths[-1]]\n\ndf = pd.DataFrame(zip(contour_set.levels, contour_polys), columns=[\"level\", \"geometry\"])\ngdf = gpd.GeoDataFrame(df, crs=brisbane.crs)\n\nclipped_kde_gdf = gdf.clip(brisbane)\n\nNow, if we plot clipped_kde_gdf, we can see that the contour has been nicely clipped to Brisbane’s boundaries.\n\nclipped_kde_gdf.plot()"
  },
  {
    "objectID": "notebooks/spatial-kde-plot.html#final-plot",
    "href": "notebooks/spatial-kde-plot.html#final-plot",
    "title": "Spatial KDE plots in Python",
    "section": "Final plot",
    "text": "Final plot\nFinally, we can plot the complete plot, this time with some background tiles.\n\nimport contextily as cx\n\nfig, ax = plt.subplots()\nbrisbane.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=0.5)\ncx.add_basemap(ax, crs=brisbane.crs, source=cx.providers.CartoDB.Voyager)\nclipped_kde_gdf.plot(\"level\", ax=ax, cmap=\"viridis\", alpha=0.7)\nax.set(xlabel=\"\", ylabel=\"\")\nplt.show()"
  }
]